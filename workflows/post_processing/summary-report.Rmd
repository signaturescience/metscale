---
title: "MetScale Summary Report"
subtitle: "MetScale: Snakemake workflows to scale the analysis of metagenomic sequences"
# author: "Stephen Turner"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    code_download: true
    code_folding: hide
    # out.width: "100%"
    fig.width: 9
    fig.height: 3
    warning: FALSE
    message: FALSE
    # theme: default
    # theme: cerulean
    # theme: journal
    # theme: flatly
    # theme: readable
    # theme: spacelab
    # theme: united
    # theme: cosmo
    # theme: lumen
    # theme: paper
    # theme: sandstone
    # theme: simplex
    # theme: yeti
    toc: yes
    toc_depth: 1
    toc_float:
      collapsed: false
      smooth_scroll: false
params:
  rmd: "report.Rmd"
---

```{r setup, message=FALSE, warning=FALSE}

# Get input dir
input_dir_param <- snakemake@params[1]
input_dir <- toString(input_dir_param)
# Set sample name (parameterized)
id <- snakemake@params[2]


# setup and load necessary libraries
library(knitr)
library(tidyverse)
library(DT)
opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
opts_knit$set(root.dir=input_dir)
theme_set(theme_bw())
```


<big>
<big>
Sample ID: <mark>**`r id`**</mark>
</big>
</big>


# About {.tabset .tabset-fade}

## Workflow overview

**More info: <https://github.com/signaturescience/metagenomics>**

These open source metagenomics workflows are intended to analyze the biological contents of complex environmental samples. The expected input is paired-end Illumina FASTQ files, and the current outputs include filtered reads, assembled contigs, MultiQC reports for FastQC and QUAST results, metagenome comparison estimates, taxonomic classifications, and gene predictions.


![Analysis flowchart](https://raw.githubusercontent.com/signaturescience/metagenomics/master/documentation/figures/Overview_Flowchart.png)


## About this report

This report compiles the results and statistics generated by version 2.0 of the MetScale workflows. Each major section presents the outputs of a specific workflow, including read filtering, assembly, comparison, taxonomic classification, and functional inference. Tabs under each major heading expand to show summary results, links to reports, and descriptions about how these results were produced. Some tabs also contain buttons that further subdivide the displayed outputs (e.g., different assemblers). Please see the wiki documentation associated with the MetScale GitHub repository for more information about the workflows, or the dependency license file for additional details about the individual open source tools run within the workflows.



# Read Filtering {.tabset .tabset-fade}



## General statistics


```{r}
get_unique_kmers <- function(file) {
  read_lines(file) %>% 
    grep("unique k-mers", ., value=TRUE) %>% 
    str_extract("\\d+") %>% 
    as.integer()
}
list.files(pattern="uniqueK\\d\\d\\.txt", recursive=TRUE) %>% 
  tibble(file=.) %>% 
  mutate(sample=str_extract(file, ".+trim\\d+")) %>% 
  mutate(K=str_extract(file, "K\\d+")) %>% 
  mutate(number_unique_kmers=map_int(file, get_unique_kmers)) %>% 
  select(-file) %>% 
  mutate_if(is.integer, scales::comma) %>% 
  DT::datatable(caption="The khmer script `unique-kmers.py` estimates the total number of unique kmers within the interleaved reads after trimming.")
```

```{r}
if (file.exists(paste0(id, "_fastqc_multiqc_report_data/multiqc_general_stats.txt"))) {
  read_tsv(paste0(id, "_fastqc_multiqc_report_data/multiqc_general_stats.txt")) %>% 
    set_names(~str_remove(., "FastQC_mqc-generalstats-")) %>% 
    mutate_if(is.double, round, 2) %>% 
    DT::datatable(caption="FastQC provides general quality control statistics about reads in the sample before and after trimming with Trimmomatic.")
}
```


## MultiQC Reports: FastQC

Click for links to comprehensive quality assessment reports:

```{r, results='asis'}
x <- list.files(pattern="fastqc_multiqc_report\\.html$", recursive=TRUE)
bx <- basename(x)
glue::glue("- **[{x}]({bx})**") %>% 
  cat(sep="\n")
```



## Detailed FastQC statistics

```{r}
if (file.exists(paste0(id, "_fastqc_multiqc_report_data/multiqc_fastqc.txt"))) {
  read_tsv(paste0(id, "_fastqc_multiqc_report_data/multiqc_fastqc.txt")) %>% 
    DT::datatable(caption="Comprehensive information from FastQC (side-scroll to see more)", options = list(scrollX='600px'))
}
```

```{r}
if (file.exists(paste0(id, "_fastqc_multiqc_report_data/multiqc_sources.txt"))) {
  read_tsv(paste0(id, "_fastqc_multiqc_report_data/multiqc_sources.txt")) %>% 
    DT::datatable(caption="MultiQC Data Sources")
}
```

# Assembly {.tabset .tabset-fade}


## General statistics {.tabset .tabset-pills}

### MEGAHIT

```{r}
if (file.exists(paste0(id, ".megahit_multiqc_report_data/multiqc_general_stats.txt"))) {
  read_tsv(paste0(id, ".megahit_multiqc_report_data/multiqc_general_stats.txt")) %>% 
    set_names(~str_remove(., "QUAST_mqc-generalstats-")) %>% 
    select(Sample, Total_length, N50) %>% 
    DT::datatable(caption="General assembly statistics about samples in this run from MEGAHIT.")
}
```

### MetaSPADES

```{r}
if (file.exists(paste0(id, ".metaspades_multiqc_report_data/multiqc_general_stats.txt"))) {
  read_tsv(paste0(id, ".metaspades_multiqc_report_data/multiqc_general_stats.txt")) %>% 
    set_names(~str_remove(., "QUAST_mqc-generalstats-")) %>% 
    select(Sample, Total_length, N50) %>% 
    DT::datatable(caption="General assembly statistics about samples in this run from metaSPAdes.")
}
```

## QUAST {.tabset .tabset-pills}

### MultiQC reports

MultiQC compiles information from all FastQC HTML reports:

```{r, results='asis'}
x <- list.files(pattern="(megahit|metaspades)_multiqc_report\\.html$", recursive=TRUE)
x1 <- file.path(getwd(), x)
glue::glue("- **[{x1}]({x1})**") %>% 
  cat(sep="\n")
```


### Individual QUAST reports

Individual QUAST reports:

```{r, results='asis'}
x <- list.files(pattern="^report\\.html$", recursive=TRUE)
x1 <- file.path(getwd(), x)
glue::glue("- **[{x1}]({x1})**") %>%  
  cat(sep="\n")
```


## Detailed results  {.tabset .tabset-pills}

### MEGAHIT

```{r}
if (file.exists(paste0(id, ".megahit_multiqc_report_data/multiqc_quast.txt"))) {
  read_tsv(paste0(id, ".megahit_multiqc_report_data/multiqc_quast.txt")) %>% 
    DT::datatable(caption="Comprehensive information from QUAST (side-scroll to see more)", options = list(scrollX='600px'))
}
```

### MetaSPADES

```{r}
if (file.exists(paste0(id, ".metaspades_multiqc_report_data/multiqc_quast.txt"))) {
  read_tsv(paste0(id, ".metaspades_multiqc_report_data/multiqc_quast.txt")) %>% 
    DT::datatable(caption="Comprehensive information from QUAST (side-scroll to see more)", options = list(scrollX='600px'))
}
```


# Comparison {.tabset .tabset-fade}

Reference database-independent sample comparisons are performed by generating MinHash signatures on filtered reads or assembled contigs with sourmash compute. The sourmash compare functionality then compares signatures from multiple datasets to each other and outputs their pairwise comparisons as Jaccard indexes, ranging from 0 (no similarity) to 1 (identical). Because many factors can contribute to sample differences (e.g., depth of sequencing), it can be difficult to draw conclusions from multi-sample comparisons. We recommend using this functionality to compare a sample to itself before and after trimming or subsampling, which will describe the k-mer content retained within the sample after processing.

## CSVs

```{r, results='asis'}
x_csv <- list.files(pattern="read_assembly_comparison.+\\.csv", recursive=TRUE)
bx_csv <- basename(x_csv)
x <- file.path(getwd(), x_csv)
glue::glue("- **[{x_csv}]({bx_csv})**")  %>% 
  cat(sep="\n")
```

## PNGs

### Direct links to images

```{r, results='asis'}
x_png <- list.files(pattern="read_assembly_comparison.+\\.png$", recursive=TRUE)
bx_png <- basename(x_png)
x_png_full <- file.path(getwd(), x_png)
glue::glue("- **[{x_png}]({bx_png})**") %>% 
  cat(sep="\n")
```


### Inline graphics (in the same order as above)

```{r}
knitr::include_graphics(x_png_full)
```

# Taxonomic Classification {.tabset .tabset-fade}


## MTSv results {.tabset .tabset-pills}

MTSv is a suite of metagenomic binning and analysis tools, and organisms are detected within a sample by identifying signature reads that are unique to a single taxon.

### Direct Links to these file:


```{r, results='asis'}
x_csv <- list.files(pattern="summary.csv$", recursive=TRUE)
bx_csv <- basename(x_csv)
x <- file.path(getwd(), x_csv)
glue::glue("- **[{x_csv}]({x})**")  %>% 
  cat(sep="\n")
```


```{r, results='asis'}
x_csv <- list.files(pattern="analysis.csv$", recursive=TRUE)
bx_csv <- basename(x_csv)
x <- file.path(getwd(), x_csv)
glue::glue("- **[{x_csv}]({x})**")  %>% 
  cat(sep="\n")
```


```{r, results='asis'}
x_csv <- list.files(pattern="binning_report.html$", recursive=TRUE)
bx_csv <- basename(x_csv)
x <- file.path(getwd(), x_csv)
glue::glue("- **[{x_csv}]({x})**")  %>% 
  cat(sep="\n")
```


```{r, results='asis'}
x_csv <- list.files(pattern="readprep_report.html$", recursive=TRUE)
bx_csv <- basename(x_csv)
x <- file.path(getwd(), x_csv)
glue::glue("- **[{x_csv}]({x})**")  %>% 
  cat(sep="\n")
```


```{r, results='asis'}
x_csv <- list.files(pattern="summary_report.html$", recursive=TRUE)
bx_csv <- basename(x_csv)
x <- file.path(getwd(), x_csv)
glue::glue("- **[{x_csv}]({x})**")  %>% 
  cat(sep="\n")
```


## Bracken results {.tabset .tabset-pills}

Bracken (Bayesian Reestimation of Abundance with KrakEN) uses the taxonomy labels assigned by Kraken2 to statistically reestimate the number of reads originating from each species present in a sample.

### Direct links to files

```{r, results='asis'}
x_png <- list.files(pattern="_bracken_db", recursive=TRUE)
bx_png <- basename(x_png)
x_png_full <- file.path(getwd(), x_png)
glue::glue("- **[{x_png}]({bx_png})**") %>% 
  cat(sep="\n")

```{r, results='asis'}
x_png <- list.files(pattern="_bracken.report$", recursive=TRUE)
bx_png <- basename(x_png)
x_png_full <- file.path(getwd(), x_png)
glue::glue("- **[{x_png}]({bx_png})**") %>% 
  cat(sep="\n")
```

## Kraken2 results {.tabset .tabset-pills}

Kraken 2 is the newest version of Kraken, which uses exact k-mer matches to short sequences and a lowest common ancestor approach to taxonomic classification.

### Direct links to files

```{r, results='asis'}
file_pattern <- "[[:print:]]{1,}(_S[[:digit:]]{1, }_L[[:digit:]]{1, }_R[[:digit:]]{1, }_[[:digit:]]{1, })?_trim[[:digit:]]{1,}_kraken2_[[:print:]]{1,}_confidence[[:digit:]]{1,}[.]report"
x_png <- list.files(pattern=file_pattern, recursive=TRUE)
bx_png <- basename(x_png)
x_png_full <- file.path(getwd(), x_png)
glue::glue("- **[{x_png}]({bx_png})**") %>% 
  cat(sep="\n")
```

## KrakenUniq results {.tabset .tabset-pills}

KrakenUniq is based on Kraken and assigns taxonomic labels to short sequences with a k-mer based lowest common ancestor approach using the original Kraken databases (not Kraken2). KrakenUniq incorporates unique k-mer counting using the HyperLogLog algorithm, and the number of unique k-mers can be found in the “kmers” column of KrakenUniq output files.

### Direct links to files

```{r, results='asis'}
file_pattern <- "[[:print:]]{1,}(_S[[:digit:]]{1, }_L[[:digit:]]{1, }_R[[:digit:]]{1, }_[[:digit:]]{1, })?_trim[[:digit:]]{1,}_krakenuniq[[:print:]]{0,}_report"
x_png <- list.files(pattern=file_pattern, recursive=TRUE)
bx_png <- basename(x_png)
x_png_full <- file.path(getwd(), x_png)
glue::glue("- **[{x_png}]({bx_png})**") %>% 
  cat(sep="\n")
```


## Mash results {.tabset .tabset-pills}

Mash uses a MinHash approach to rapidly compare large sequence datasets. The first column of the mash screen output is `identity`, which describes the fraction of k-mers in a reference genome that also appear in a metagenome, ranging from 0 to 1. The authors of Mash recommend 0.90 as an `identity` threshold for detecting the presence of a reference genome within a metagenome. Mash distance can be used to determine the reference genome that most closely matches an isolate genome sequence. Within the MetScale workflows, the Mash default reference database is derived from NCBI RefSeq assemblies, but users can also run mash with a custom reference database.

### Direct links to files

```{r, results='asis'}
x_png <- list.files(pattern="_mash_distances", recursive=TRUE)
bx_png <- basename(x_png)
x_png_full <- file.path(getwd(), x_png)
glue::glue("- **[{x_png}]({bx_png})**") %>% 
  cat(sep="\n")
```



## Kaiju results {.tabset .tabset-pills}

 Kaiju assigns taxonomic labels to short sequences with a lowest common ancestor approach and a reference database of microbial protein sequences. Kaiju also includes functionalities to filter results and display them in krona plots.

### Links to existing HTML files

Click for links to krona diagrams:

```{r, results='asis'}
x <- list.files(pattern=".*krona.*html$", recursive=TRUE)
bx <- basename(x)
glue::glue("- **[{x}]({bx})**") %>% 
  cat(sep="\n")
```

### Direct Krona embedding 
```{r, results='asis'}
x <- list.files(pattern=".*krona.*html$", recursive=TRUE)
bx <- basename(x)
  paste0("\n", bx, ":\n\n", '<iframe src="', bx, '" width="1000" height="1000"></iframe>\n\n') %>% 
  cat()
```

### Direct links to files


```{r, results='asis'}
x <- list.files(pattern="kaiju.+summary$", recursive=TRUE)
bx <- basename(x)
glue::glue("- **[{x}]({bx})**") %>% 
  cat(sep="\n")
```

### Inline reports

For brevity, the inline reports shown here only display the top 50 results for each sample. Please click the _"Direct links to files"_ button above to download and view the full results for each sample.

```{r}
# readLines(x[1]) %>% 
#   cat(sep="\n")
for (i in x) {
  cat("\n", i, "\n\n")
  cat(readLines(i, n=52), sep="\n")
  cat("\n\n\n\n")
}
```

## Sourmash Results {.tabset .tabset-pills}

Sourmash was inspired by Mash and implements a similar MinHash approach. Sourmash gather will search NCBI RefSeq and GenBank databases to report the most highly contained microbial genomes within a query metagenome. The `f_match` is the fraction of the reference genome match shared with the query metagenome, ranging from 0 to 1. Note that mash screen and sourmash gather are powerful for detecting the presence of a reference genome within a metagenome, but they do not report the exact boundaries of the reference genome match.

### Direct links to spreadsheets

```{r, results='asis'}
x <- list.files(pattern="gather_output.csv$", recursive=TRUE)
bx <- basename(x)
glue::glue("- **[{x}]({bx})**") %>% 
  cat(sep="\n")
```


### Trim2 k=21

```{r}
if (file.exists(paste0(id, "_trim2_k21.gather_output.csv"))) {
  read_csv(paste0(id, "_trim2_k21.gather_output.csv")) %>% 
    mutate_if(is.double, round, 2) %>% 
    DT::datatable(caption="Sourmash Gather results for k=21 (side-scroll to see more)", options = list(scrollX='600px'))
}
```

### Trim2 k=31

```{r}
if (file.exists(paste0(id, "_trim2_k31.gather_output.csv"))) {
  read_csv(paste0(id,"_trim2_k31.gather_output.csv")) %>% 
    mutate_if(is.double, round, 2) %>% 
    DT::datatable(caption="Sourmash Gather results for k=31 (side-scroll to see more)", options = list(scrollX='600px'))
}
```

### Trim2 k=51

```{r}
if (file.exists(paste0(id, "_trim2_k51.gather_output.csv"))) {
  read_csv(paste0(id,"_trim2_k51.gather_output.csv")) %>% 
    mutate_if(is.double, round, 2) %>% 
    DT::datatable(caption="Sourmash Gather results for k=51 (side-scroll to see more)", options = list(scrollX='600px'))
}
```

### Trim30 k=21

```{r}
if (file.exists(paste0(id, "_trim30_k21.gather_output.csv"))) {
read_csv(paste0(id,"_trim30_k21.gather_output.csv")) %>% 
  mutate_if(is.double, round, 2) %>% 
  DT::datatable(caption="Sourmash Gather results for k=21 (side-scroll to see more)", options = list(scrollX='600px'))
}
```

### Trim30 k=31

```{r}
if (file.exists(paste0(id, "_trim30_k31.gather_output.csv"))) {
  read_csv(paste0(id,"_trim30_k31.gather_output.csv")) %>% 
    mutate_if(is.double, round, 2) %>% 
    DT::datatable(caption="Sourmash Gather results for k=31 (side-scroll to see more)", options = list(scrollX='600px'))
}
```

### Trim30 k=51

```{r}
if (file.exists(paste0(id, "_trim30_k51.gather_output.csv"))) {
  read_csv(paste0(id,"_trim30_k51.gather_output.csv")) %>% 
    mutate_if(is.double, round, 2) %>% 
    DT::datatable(caption="Sourmash Gather results for k=51 (side-scroll to see more)", options = list(scrollX='600px'))
}
```

# Functional Inference {.tabset .tabset-fade}

## Prokka

Prokka performs de novo annotations of microbial open reading frames within isolates or metagenomes.

Click below to access prokka results:
```{r, results='asis'}
x <- list.dirs() %>% 
  grep(pattern = "prokka_annotation", x=., value=TRUE) %>% 
  list.files(path=., pattern="\\.tsv", full.names = TRUE, recursive=TRUE)
x1 <- file.path(getwd(), x)
bx <- basename(x)
glue::glue("- **[{x1}]({x1})**") %>% 
  cat(sep="\n")
```

## Antibiotic resistance {.tabset .tabset-pills}

ABRicate was designed to screen contigs for antimicrobial resistance or virulence genes

### Abricate (contigs)

ABRicate was designed to screen contigs for antimicrobial resistance or virulence genes

```{r, results='asis'}
x <- list.files(pattern="abricate.*\\.csv$", recursive=TRUE)
bx <- basename(x)
glue::glue("- **[{x}]({bx})**") %>% 
  cat(sep="\n")
```

### SRST2 (reads)

SRST2 (Short Read Sequence Typing for Bacterial Pathogens) was designed to report the presence of a list of reference genes, such as those responsible for antibiotic resistance, from short Illumina reads

```{r, results='asis'}
x <- list.files(pattern="srst2.*results\\.txt$", recursive=TRUE)
bx <- basename(x)
glue::glue("- **[{x}]({bx})**") %>% 
  cat(sep="\n")
```




```{r, include=FALSE}
knitr::knit_exit()
```


# xx {.tabset .tabset-fade}

## General statistics

## Detailed results

## Procedure

Information goes here about the procedure, versions, etc. Lorem ipsum dolor sit amet, semper suscipit sea at. Dico scriptorem nec at, ex qui virtute dolores oportere. Duis tantas ponderum ut has. Est saepe mandamus salutatus et, id sed semper detracto moderatius, ei sit aperiam voluptua. Per esse justo fierent eu, duo quando tempor ut. At elitr doming possim vim, ut dolorem appetere nec.





# xx {.tabset .tabset-fade}

## General statistics

## Detailed results

## Procedure

Information goes here about the procedure, versions, etc. Lorem ipsum dolor sit amet, semper suscipit sea at. Dico scriptorem nec at, ex qui virtute dolores oportere. Duis tantas ponderum ut has. Est saepe mandamus salutatus et, id sed semper detracto moderatius, ei sit aperiam voluptua. Per esse justo fierent eu, duo quando tempor ut. At elitr doming possim vim, ut dolorem appetere nec.



