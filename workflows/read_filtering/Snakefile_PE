'''
Author: Phillip Brooks, Charles Reid, Chris Grahlmann
Affiliation: UC Davis Lab for Data Intensive Biology
Objective: A Snakemake workflow to process reads to produce quality trimmed data 
Date: 2018-06-08
Documentation: docs/workflow_readfilt.md
'''

from common.utils  import container_image_is_external, container_image_name, get_samples_filenames
from os.path import join, isfile, dirname
import os, re, gzip, io, glob, shutil
import pdb
from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider

HTTP = HTTPRemoteProvider()



###################################
# Read Filtering: default config

data_dir = config['data_dir']
sing_dir = config['sing_dir']
biocontainers = config['biocontainers']
taxclass = config['taxonomic_classification']
assembly = config['assembly']
readfilt = config['read_filtering']
post_processing = config['post_processing']
workflows = config['workflows']

###################################
# Read Filtering: build rules

# Skip to the very end of the file 
# to see the high-level build rules
# that trigger cascades of workflow
# tasks.



###################################
# Code to get samples input files based on sample name and glob file pattern
'''

all_sample_names = workflows['samples_input_workflow']['samples']
#sample_input_files = get_sample_inputs(all_sample_names, readfilt['read_patterns']['pre_trimming_glob_pattern'], readfilt["quality_trimming"]["sample_file_ext"], data_dir)
sample_input_files, sample_finished_folders = get_samples_filenames()
if (sample_input_files == []):
    print("Warning: No input files found!")
'''
'''
def get_samples_filenames():
    # Code to get samples input files based on sample name and glob file pattern
    #Using user defined params get input file list
    
    all_sample_names = workflows['samples_input_workflow']['samples']
    sample_input_files = []
    sample_finished_folders = [] # Also checking for data subfolders with '_finished' appended
    for sample in all_sample_names:
        input_file_pattern = readfilt['read_patterns']['pre_trimming_glob_pattern']
        input_file_pattern = re.sub(r"\*", sample, input_file_pattern, 1)
        input_file_pattern = join(data_dir, input_file_pattern)
        finished_folder_output_pattern = post_processing['move_samples_to_dir']['output_pattern'].replace('{sample}','')
        finished_folder_pattern = input_file_pattern.replace(readfilt["quality_trimming"]["sample_file_ext"], finished_folder_output_pattern) 
        sample_input_files.extend(glob.glob(input_file_pattern))
        sample_finished_folders.extend(glob.glob(finished_folder_pattern))

    #now we need to remove file extension and data_dir
    file_names = []
    for file in sample_input_files:
        file_path_no_dir = file.replace(data_dir+'/','')
        file_names.append(file_path_no_dir.replace(readfilt["quality_trimming"]["sample_file_ext"],''))
    sample_input_files = file_names
    if (sample_input_files == [] and sample_finished_folders == []):
        print("Warning: No input files found!")
    return sample_input_files, sample_finished_folders
'''
    
sample_input_files, sample_finished_folders = get_samples_filenames(workflows, readfilt, post_processing, data_dir)
###################################
# Pretrimming code needed for convert_illumina_to_internal_format and pre_trimming_quality_assessment


fq_fwd_pre_trimmed = join(data_dir, (readfilt['read_patterns']['pre_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                        direction=readfilt['direction_labels']['forward'],
                        sample='{sample}'))
fq_rev_pre_trimmed = join(data_dir, (readfilt['read_patterns']['pre_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                        direction=readfilt['direction_labels']['reverse'],
                        sample='{sample}'))

fq_fwd_pre_trimmed_sing = join(sing_dir, (readfilt['read_patterns']['pre_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                        direction=readfilt['direction_labels']['forward'],
                        sample='{sample}'))

fq_rev_pre_trimmed_sing = join(sing_dir, (readfilt['read_patterns']['pre_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                        direction=readfilt['direction_labels']['reverse'],
                        sample='{sample}'))

                     
pre_trimming_inputs_sing = [ fq_fwd_pre_trimmed_sing, 
                        fq_rev_pre_trimmed_sing]
                        
pre_trimming_inputs = [ fq_fwd_pre_trimmed, 
                        fq_rev_pre_trimmed]


###################################
# Convert external files to internal file formats


def convert_illumina_input_files(wildcards):
    input_file = join(data_dir, wildcards.sample + readfilt["quality_trimming"]["sample_file_ext"])
    return input_file
    
def create_output_internal_format_fwd(wildcards):
   return fq_fwd_pre_trimmed.format(**wildcards)

def create_output_internal_format_rev(wildcards):
   return fq_rev_pre_trimmed.format(**wildcards)


rule convert_external_to_internal_format:
    '''
    Convert the external file format to our internal file format
    '''
    input:
        convert_illumina_input_files
    output:
        pre_trimming_inputs
    params:
        dest_fwd = create_output_internal_format_fwd,
        dest_rev = create_output_internal_format_rev,
        search_pattern = readfilt["read_patterns"]["reverse_pe_pattern_search"],
        replace_pattern = readfilt["read_patterns"]["reverse_pe_pattern_replace"],
    run:
        src_fwd_str = str(input)
        src_rev_str =  params.replace_pattern.join(src_fwd_str.rsplit(params.search_pattern, 1))
        shutil.copy(src_fwd_str, params.dest_fwd)
        shutil.copy(src_rev_str, params.dest_rev)






###################################
# Read Filtering: pre trimming

trimmed_threads = readfilt['read_patterns']['threads']

target_suffix = readfilt['quality_assessment']['fastqc_suffix']
target_ext = "_%s.zip"%(target_suffix)

sample_file_ext = readfilt['quality_trimming']['sample_file_ext']


pre_trimming_output_fwd = re.sub(sample_file_ext, target_ext, fq_fwd_pre_trimmed)
pre_trimming_output_rev = re.sub(sample_file_ext, target_ext, fq_rev_pre_trimmed)

pre_trimming_output_fwd_sing = re.sub(sample_file_ext, target_ext, fq_fwd_pre_trimmed_sing)
pre_trimming_output_rev_sing = re.sub(sample_file_ext, target_ext, fq_rev_pre_trimmed_sing)

pre_trimming_outputs = [pre_trimming_output_fwd,
                        pre_trimming_output_rev]

pre_trimming_outputs_sing = [pre_trimming_output_fwd_sing,
                        pre_trimming_output_rev_sing]

fastqc_image = container_image_name(biocontainers, 'fastqc')

def pre_trimming_qa_inputs(wildcards):
    # input already includes data/ prefix
    pre_inputs_wc = [x.format(**wildcards) for x in pre_trimming_inputs_sing]

    # for containers, turn prefix data/ into /data/
    pre_inputs_wc = ["/%s"%(x) for x in pre_inputs_wc]
    pre_inputs_wcs = " ".join(pre_inputs_wc)
    return pre_inputs_wc

def pre_trimming_qa_outputs(wildcards):
    # output already includes data/ prefix
    pre_outputs_wc = [x.format(**wildcards) for x in pre_trimming_outputs_sing]

    # for containers, turn prefix data/ into /data/
    pre_outputs_wc = ["/%s"%(x) for x in pre_outputs_wc]
    pre_outputs_wcs = " ".join(pre_outputs_wc)
    return pre_outputs_wcs


rule pre_trimming_quality_assessment:
    """
    Perform a pre-trimming quality check of the reads from the sequencer.
    """
    input:
        pre_trimming_inputs
    output: 
        pre_trimming_outputs
    message: 
        '''--- Pre-trim quality check of trimmed data with fastqc.'''
    singularity: 
        fastqc_image
    threads: 
        trimmed_threads
    params:
        pre_trimming_output_wc = pre_trimming_qa_outputs,
        pre_trimming_input_wc = pre_trimming_qa_inputs
    shell:
        'fastqc -t {threads} '
        '/{params.pre_trimming_input_wc} '
        '-o /{sing_dir} '


###################################
## Read Filtering: Four rules to convert PE to SE

flash_merged_pair_out_extendedFrags = join(data_dir, (readfilt['convert_pe_to_se']['flash_pe_output_extendedFrags'].format(sample='{sample}')))
flash_merged_pair_out_notCombined_1 =  join(data_dir, (readfilt['convert_pe_to_se']['flash_pe_output_notCombined_1'].format(sample='{sample}')))
flash_merged_pair_out_notCombined_2 =  join(data_dir, (readfilt['convert_pe_to_se']['flash_pe_output_notCombined_2'].format(sample='{sample}')))
flash_merged_pair_out_sing = (readfilt['convert_pe_to_se']['flash_pe_output_sing'].format(sample='{sample}'))

flash_merged_pair_log = join(sing_dir, (readfilt['convert_pe_to_se']['flash_pe_log'].format(sample='{sample}')))
flash_image = container_image_name(biocontainers, 'flash')

flash_merge_pair_output = [flash_merged_pair_out_extendedFrags, flash_merged_pair_out_notCombined_1, flash_merged_pair_out_notCombined_2]

def flash_merge_pair_output_sing(wildcards):
    return flash_merged_pair_out_sing.format(**wildcards)

#Rule 1
rule merge_pair_ends_with_flash:
    input:
        pre_trimming_inputs
    output:
        flash_merge_pair_output
    singularity:
        flash_image
    params:
        merged_pair_input = pre_trimming_qa_inputs,
        merged_pair_output = flash_merge_pair_output_sing,
        merged_pair_log = flash_merged_pair_log
    shell:
        'flash {params.merged_pair_input} -M 70 -o {params.merged_pair_output} '
        '-d {sing_dir} 2>&1 | tee {params.merged_pair_log}'

#############################################

merged_reads_with_trim_SE_input = join(data_dir, (readfilt['convert_pe_to_se']['trim_se_trim_input'].format(sample='{sample}', qual='{qual}')))
merged_reads_trim_SE_output = join(data_dir, (readfilt['convert_pe_to_se']['trim_se_trim_output'].format(sample='{sample}', qual='{qual}')))
trimmo_image = container_image_name(biocontainers,'trimmomatic')
trimmmo_threads = readfilt['adapter_file']['threads']

def merged_reads_with_trimmomatic_SE_out_sing(wildcards):
    return join(sing_dir, (readfilt['convert_pe_to_se']['trim_se_trim_output'].format(**wildcards)))

def merged_reads_with_trimmomatic_SE_input_sing(wildcards):
    return join(sing_dir, (readfilt['convert_pe_to_se']['flash_pe_output_extendedFrags'].format(**wildcards)))

def quality_trimming_qual(wildcards):
    """Get quality threshold for trimming"""
    return "{qual}".format(**wildcards)
    

#Rule 2
rule merged_reads_with_trimmomatic_SE:
    input:
        merged_reads_with_trim_SE_input
    output:
        merged_reads_trim_SE_output
    threads:
        trimmmo_threads
    singularity:
        trimmo_image
    params:
        qual = quality_trimming_qual,
        trim_SE_output = merged_reads_with_trimmomatic_SE_out_sing,
        trim_SE_input =  merged_reads_with_trimmomatic_SE_input_sing
    shell:
        'trimmomatic SE '
        '{params.trim_SE_input} '
        '{params.trim_SE_output} '
        'ILLUMINACLIP:{sing_dir}/adapters_combined_256_unique.fasta:2:30:15:1:true '
        'LEADING:{params.qual} '
        'TRAILING:{params.qual} '
        'SLIDINGWINDOW:4:{params.qual} '
        'MINLEN:25 '
        '-threads {threads} '
        

#############################################

merged_reads_trim_PE_input = [flash_merged_pair_out_notCombined_1, flash_merged_pair_out_notCombined_2]
flash_merged_pair_out_notCombined_1_sing = join(sing_dir, (readfilt['convert_pe_to_se']['flash_pe_output_notCombined_1'].format(sample='{sample}')))
flash_merged_pair_out_notCombined_2_sing = join(sing_dir, (readfilt['convert_pe_to_se']['flash_pe_output_notCombined_2'].format(sample='{sample}')))
merged_reads_trim_PE_input_sing = [flash_merged_pair_out_notCombined_1_sing, flash_merged_pair_out_notCombined_2_sing]

merged_reads_trimm_PE_out_tr_fastq_1 = join(data_dir, (readfilt['convert_pe_to_se']['flash_se_output_notCombined_tr_fastq_1'].format(sample='{sample}', qual='{qual}')))
merged_reads_trimm_PE_out_tr_fastq_2 = join(data_dir, (readfilt['convert_pe_to_se']['flash_se_output_notCombined_tr_fastq_2'].format(sample='{sample}', qual='{qual}')))
merged_reads_trimm_PE_out_tr_se_1 = join(data_dir, (readfilt['convert_pe_to_se']['flash_se_output_notCombined_tr_se_1'].format(sample='{sample}', qual='{qual}')))
merged_reads_trimm_PE_out_tr_se_2 = join(data_dir, (readfilt['convert_pe_to_se']['flash_se_output_notCombined_tr_se_2'].format(sample='{sample}', qual='{qual}''')))
            
merged_reads_trim_PE_output = [merged_reads_trimm_PE_out_tr_fastq_1, merged_reads_trimm_PE_out_tr_fastq_2, merged_reads_trimm_PE_out_tr_se_1, merged_reads_trimm_PE_out_tr_se_2]

merged_reads_trimm_PE_out_tr_fastq_1_sing = join(sing_dir, (readfilt['convert_pe_to_se']['flash_se_output_notCombined_tr_fastq_1'].format(sample='{sample}', qual='{qual}')))
merged_reads_trimm_PE_out_tr_fastq_2_sing = join(sing_dir, (readfilt['convert_pe_to_se']['flash_se_output_notCombined_tr_fastq_2'].format(sample='{sample}', qual='{qual}')))
merged_reads_trimm_PE_out_tr_se_1_sing = join(sing_dir, (readfilt['convert_pe_to_se']['flash_se_output_notCombined_tr_se_1'].format(sample='{sample}', qual='{qual}')))
merged_reads_trimm_PE_out_tr_se_2_sing = join(sing_dir, (readfilt['convert_pe_to_se']['flash_se_output_notCombined_tr_se_2'].format(sample='{sample}', qual='{qual}')))

merged_reads_trim_PE_out_sing = [merged_reads_trimm_PE_out_tr_fastq_1_sing, merged_reads_trimm_PE_out_tr_se_1_sing,
        merged_reads_trimm_PE_out_tr_fastq_2_sing, merged_reads_trimm_PE_out_tr_se_2_sing]




#rule 3
rule filter_merged_reads_trimmomatic_PE:
    input:
        merged_reads_trim_PE_input
    output:
        merged_reads_trim_PE_output
    threads:
        trimmmo_threads
    singularity:
        trimmo_image
    params:
        qual = quality_trimming_qual,
        trim_PE_input = merged_reads_trim_PE_input_sing,
        trim_PE_out = merged_reads_trim_PE_out_sing
    shell:
        'trimmomatic PE '
        '{params.trim_PE_input} '
        '{params.trim_PE_out} '
        'ILLUMINACLIP:{sing_dir}/adapters_combined_256_unique.fasta:2:30:15:1:true '
        'LEADING:{params.qual} '
        'TRAILING:{params.qual} '
        'SLIDINGWINDOW:4:{params.qual} '
        'MINLEN:25 '
        '-threads {threads}'


#############################################
combine_read_trim_input = [merged_reads_trim_SE_output, merged_reads_trimm_PE_out_tr_fastq_1, 
                           merged_reads_trimm_PE_out_tr_se_1, merged_reads_trimm_PE_out_tr_se_2]
combine_read_trim_output = join(data_dir, (readfilt['convert_pe_to_se']['combine_reads_compress_output'].format(sample='{sample}', qual='{qual}')))

#rule 4
rule combine_PE_reads_outputs_and_compress:
    input:
        combine_read_trim_input
    output:
        combine_read_trim_output
    params:

    shell:
        'gzip -c {input} > {output}'



###################################
# Read Filtering: post trimming


fq_fwd_post_trimmed = join(data_dir, (readfilt['read_patterns']['post_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                            direction=readfilt['direction_labels']['forward'],
                            sample='{sample}',
                            qual='{qual}'))
fq_rev_post_trimmed = join(data_dir, (readfilt['read_patterns']['post_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                            direction=readfilt['direction_labels']['reverse'],
                            sample='{sample}',
                            qual='{qual}'))
                            
fq_fwd_post_trimmed_sing = join(sing_dir, (readfilt['read_patterns']['post_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                            direction=readfilt['direction_labels']['forward'],
                            sample='{sample}',
                            qual='{qual}'))

fq_rev_post_trimmed_sing = join(sing_dir, (readfilt['read_patterns']['post_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                            direction=readfilt['direction_labels']['reverse'],
                            sample='{sample}',
                            qual='{qual}'))
                        

post_trimming_inputs_sing = [fq_fwd_post_trimmed_sing, fq_rev_post_trimmed_sing]
post_trimming_inputs = [fq_fwd_post_trimmed, fq_rev_post_trimmed]

target_suffix = readfilt['quality_assessment']['fastqc_suffix']
target_ext = "_%s.zip"%(target_suffix)

sample_file_ext = readfilt['quality_trimming']['sample_file_ext']
post_trimming_output_fwd = re.sub(sample_file_ext, target_ext, fq_fwd_post_trimmed)
post_trimming_output_rev = re.sub(sample_file_ext, target_ext, fq_rev_post_trimmed)

post_trimming_outputs = [post_trimming_output_fwd,
                         post_trimming_output_rev]

fastqc_image = container_image_name(biocontainers, 'fastqc')


def post_trimming_qa_inputs_sing(wildcards):
    # input already includes data/ prefix
    post_inputs_wc = [x.format(**wildcards) for x in post_trimming_inputs_sing]

    # for containers, turn prefix data/ into /data/
    post_inputs_wc = ["/%s"%(x) for x in post_inputs_wc]
    post_inputs_wcs = " ".join(post_inputs_wc)
    return post_inputs_wc

def post_trimming_qa_outputs(wildcards):
    # output already includes data/ prefix
    post_outputs_wc = [x.format(**wildcards) for x in post_trimming_outputs]

    # for containers, turn prefix data/ into /data/
    post_outputs_wc = ["/%s"%(x) for x in post_outputs_wc]
    post_outputs_wcs = " ".join(post_outputs_wc)
    return post_outputs_wcs


rule post_trimming_quality_assessment:
    """
    Perform a post-trimming quality check 
    of the reads from the sequencer.
    """
    input:
        post_trimming_inputs
    output:
        post_trimming_outputs
    message: 
        '''--- Post-trim quality check of trimmed data with fastqc.'''
    singularity: 
        fastqc_image
    threads: 
        trimmed_threads 
    params:
        post_trimming_outputs_wc = post_trimming_qa_outputs,
        post_trimming_inputs_wc = post_trimming_qa_inputs_sing
    shell:
        'fastqc -t {threads} '
        '{params.post_trimming_inputs_wc} '
        '-o /{sing_dir} '




###################################
# Read Filtering: interleave

# These are strings containing templates of .fq.gz file names.
fq_fwd_trimmed = join(data_dir, (readfilt['read_patterns']['post_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                        direction=readfilt['direction_labels']['forward'],
                        sample='{sample}',
                        qual='{qual}'))

fq_rev_trimmed = join(data_dir, (readfilt['read_patterns']['post_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                        direction=readfilt['direction_labels']['reverse'],
                        sample='{sample}',
                        qual='{qual}'))

fq_fwd_trimmed_sing = join(sing_dir, (readfilt['read_patterns']['post_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                        direction=readfilt['direction_labels']['forward'],
                        sample='{sample}',
                        qual='{qual}'))

fq_rev_trimmed_sing = join(sing_dir, (readfilt['read_patterns']['post_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                        direction=readfilt['direction_labels']['reverse'],
                        sample='{sample}',
                        qual='{qual}'))

fq_interleave  = join(data_dir, readfilt['interleaving']['interleave_output_pattern'].format(
                        sample='{sample}',
                        qual='{qual}'))
fq_interleave_sing  = join(sing_dir, readfilt['interleaving']['interleave_output_pattern'].format(
                        sample='{sample}',
                        qual='{qual}'))


interleave_input = [fq_fwd_trimmed, fq_rev_trimmed]
interleave_input_sing = [fq_fwd_trimmed_sing, fq_rev_trimmed_sing]
interleave_output = fq_interleave
interleave_output_sing = fq_interleave_sing

khmer_image = container_image_name(biocontainers,'khmer')


def interleave_reads_inputs_sing(wildcards):
    # inputs should include the data directory prefix
    # do wildcard substitution
    interleave_input_wc = [x.format(**wildcards) for x in interleave_input_sing]

    # for docker container, turn prefix 'data/' into '/data/'
    interleave_input_wc = ["/%s"%(x) for x in interleave_input_wc]
    interleave_input_wcs = " ".join(interleave_input_wc)
    return interleave_input_wcs

def interleave_reads_inputs(wildcards):
    # inputs should include the data directory prefix
    # do wildcard substitution
    interleave_input_wcs = [x.format(**wildcards) for x in interleave_input]
    return interleave_input_wcs

def interleave_reads_output_sing(wildcards):
    # wildcard substitution
    return interleave_output_sing.format(**wildcards)

def interleave_reads_logfile(wildcards):
    # wildcard substitution
    return interleave_reads_log.format(**wildcards)


rule interleave_reads:
    """
    Interleave paired-end reads using khmer.
    The trim quality comes from the filename.
    """
    input:
        interleave_input = interleave_reads_inputs
    output:
        interleave_output
    message:
        """--- Interleaving read data."""
    singularity:
        khmer_image
    params:
        interleave_input_wc = interleave_reads_inputs_sing,
        interleave_output_wc = interleave_reads_output_sing
    shell:
        'interleave-reads.py {params.interleave_input_wc} '
        '--no-reformat '
        '-o {params.interleave_output_wc} '
        '--gzip'



###################################
# Read Filtering: sub set interleave

subsample_interleave_input = interleave_output
subsample_interleave_input_sing = interleave_output_sing
subsample_interleave_max_reads = readfilt['subsample_interleaving']['max_reads']
subsample_interleave_percent = readfilt['subsample_interleaving']['percent']

subsample_interleave_output = join(data_dir, readfilt['subsample_interleaving']['subsample_output_pattern'].format(
    percent=readfilt['subsample_interleaving']['percent'],
    sample='{sample}', qual='{qual}'))
subsample_interleave_output_sing = join(sing_dir, readfilt['subsample_interleaving']['subsample_output_pattern'].format(
    percent=readfilt['subsample_interleaving']['percent'],
    sample='{sample}', qual='{qual}'))
    


def subsample_interleave_reads_inputs(wildcards):
    return subsample_interleave_input.format(**wildcards)

def subsample_interleave_reads_inputs_sing(wildcards):
    return subsample_interleave_input_sing.format(**wildcards)

def subsample_interleave_reads_output_sing(wildcards):
    return subsample_interleave_output_sing.format(**wildcards)


rule subsample_interleaved_reads:
    """
    Sample from full interleaved reads
    """
    input:
        subsample_interleave_input
    output:
        subsample_interleave_output
    message:
        """--- Subsample Interleaved read data."""
    singularity:
        khmer_image
    params:
        subsample_interleave_input_wc =  subsample_interleave_reads_inputs_sing,
        subsample_interleave_output_wc = subsample_interleave_reads_output_sing,
    shell:
        'TOTAL_READS="$(zcat {params.subsample_interleave_input_wc} | echo $((`wc -l`/4)))" && '
        'NUM_READS=$((TOTAL_READS / {subsample_interleave_percent})) && '
        'sample-reads-randomly.py -N $NUM_READS '
        '-M {subsample_interleave_max_reads} '
        '-o {params.subsample_interleave_output_wc} '
        '--gzip {params.subsample_interleave_input_wc} '





###################################
# Read Filtering: split interleaved reads

split_interleave_input = subsample_interleave_output
split_interleave_input_sing = subsample_interleave_output_sing


split_interleave_output_rev = join(data_dir, (readfilt['split_interleaved_reads']['split_interleaved_output_pattern']).format(
                        direction=readfilt['direction_labels']['reverse'],
                        percent=readfilt['subsample_interleaving']['percent'],
                        sample='{sample}',
                        qual='{qual}'))
split_interleave_output_fwd = join(data_dir, (readfilt['split_interleaved_reads']['split_interleaved_output_pattern']).format(
                        direction=readfilt['direction_labels']['forward'],
                        percent=readfilt['subsample_interleaving']['percent'],
                        sample='{sample}',
                        qual='{qual}'))

split_interleave_output_rev_sing = join(sing_dir, (readfilt['split_interleaved_reads']['split_interleaved_output_pattern']).format(
                        direction=readfilt['direction_labels']['reverse'],
                        percent=readfilt['subsample_interleaving']['percent'],
                        sample='{sample}',
                        qual='{qual}'))
split_interleave_output_fwd_sing = join(sing_dir, (readfilt['split_interleaved_reads']['split_interleaved_output_pattern']).format(
                        direction=readfilt['direction_labels']['forward'],
                        percent=readfilt['subsample_interleaving']['percent'],
                        sample='{sample}',
                        qual='{qual}'))

split_interleave_output = [split_interleave_output_fwd , split_interleave_output_rev ]


def split_interleaved_qa_outputs_fwd_sing(wildcards):
    return split_interleave_output_fwd_sing.format(**wildcards)

def split_interleaved_qa_outputs_rev_sing(wildcards):
    return split_interleave_output_rev_sing.format(**wildcards)


rule split_interleaved_reads:
    input:
        split_interleave_input
    output:
        split_interleave_output 
    message:
       """--- Split interleaved reads."""
    singularity:
        khmer_image
    params:
        split_input = split_interleave_input_sing,
        output_fwd = split_interleaved_qa_outputs_fwd_sing,
        output_rev = split_interleaved_qa_outputs_rev_sing,
    shell:
        'split-paired-reads.py {params.split_input} '
        '-1 {params.output_fwd} '
        '-2 {params.output_rev} --gzip '


###################################
# Read Filtering: count unique reads

count_unique_reads_input = join(data_dir, readfilt['count_unique_reads']['input_pattern'].format(
                        sample='{sample}', qual='{qual}'))
count_unique_reads_input_sing = join(sing_dir, readfilt['count_unique_reads']['input_pattern'].format(
                        sample='{sample}', qual='{qual}'))


count_unique_reads_output =  join(data_dir, readfilt['count_unique_reads']['output_pattern'])
count_unique_reads_output_sing =  join(sing_dir, readfilt['count_unique_reads']['output_pattern'])


rule count_unique_reads:
    input:
      count_unique_reads_input
    output:
      count_unique_reads_output
    message:
        """--- Count unique reads."""
    singularity:
        khmer_image 
    params:
        reads_input = count_unique_reads_input_sing,
        reads_output = count_unique_reads_output_sing
    shell:
        'unique-kmers.py -k {wildcards.kmers} '
        '{params.reads_input} '
        '-R {params.reads_output}'



###################################
# Read Filtering: convert fastq files to fasta

files, = glob_wildcards("data/{file}.fq.gz")

fastq_to_fasta_input = join(data_dir, readfilt['convert_fastq_fasta']['input_pattern'])
fastq_to_fasta_input_sing = join(sing_dir,readfilt['convert_fastq_fasta']['input_pattern'])
fastq_to_fasta_output = join(data_dir,readfilt['convert_fastq_fasta']['output_pattern'])
fastq_to_fasta_output_sing = join(sing_dir,readfilt['convert_fastq_fasta']['output_pattern'])

def convert_fastq_to_fasta_input_sing(wildcards):
    return fastq_to_fasta_input_sing.format(**wildcards)

def convert_fast_to_fasta_output_sing(wildcards):
    return fastq_to_fasta_output_sing.format(**wildcards)


rule convert_fastq_to_fasta:
    input:  
        fastq_to_fasta_input
    output: 
        fastq_to_fasta_output
    message: 
        """--- Convert fastq to fasta."""
    singularity:
        khmer_image
    params:
        convert_input = convert_fastq_to_fasta_input_sing,
        convert_output = convert_fast_to_fasta_output_sing
    shell:  
        "fastq-to-fasta.py -o {params.convert_output} {params.convert_input}"


###################################
# Read Filtering: quality trimming
# If you only want to substitute a subset of wildcards,
# you can leave a wildcard untouched by substituting
# the string {variable} for {variable}.
# 
# We use this trick several times in these rules.
# 

fq_fwd = join(data_dir, (readfilt['read_patterns']['pre_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                    direction=readfilt['direction_labels']['forward'],
                    sample='{sample}'))

fq_rev = join(data_dir, (readfilt['read_patterns']['pre_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                    direction=readfilt['direction_labels']['reverse'],
                    sample='{sample}'))
                    
fq_fwd_sing = join(sing_dir, (readfilt['read_patterns']['pre_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                    direction=readfilt['direction_labels']['forward'],
                    sample='{sample}'))
            
fq_rev_sing = join(sing_dir, (readfilt['read_patterns']['pre_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                    direction=readfilt['direction_labels']['reverse'],
                    sample='{sample}'))

quality_input = [fq_fwd, fq_rev]
quality_input_sing = [fq_fwd_sing, fq_rev_sing]

fq_fwd_trimmed_sing = join(sing_dir, (readfilt['read_patterns']['post_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                        direction=readfilt['direction_labels']['forward'],
                        sample='{sample}',
                        qual='{qual}'))

fq_rev_trimmed_sing = join(sing_dir, (readfilt['read_patterns']['post_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                        direction=readfilt['direction_labels']['reverse'],
                        sample='{sample}',
                        qual='{qual}'))

fq_fwd_trimmed = join(data_dir, (readfilt['read_patterns']['post_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                        direction=readfilt['direction_labels']['forward'],
                        sample='{sample}',
                        qual='{qual}'))

fq_rev_trimmed = join(data_dir, (readfilt['read_patterns']['post_trimming_pattern']+readfilt['quality_trimming']['sample_file_ext']).format(
                        direction=readfilt['direction_labels']['reverse'],
                        sample='{sample}',
                        qual='{qual}'))
                      
trim_target_ext = readfilt['quality_trimming']['trim_suffix']
sample_file_ext = readfilt['quality_trimming']['sample_file_ext']
trimmmo_threads = readfilt['adapter_file']['threads']

#fq_fwd_se = re.sub('\.fq\.gz',"_%s"%(trim_target_ext),fq_fwd_trimmed)
#fq_rev_se = re.sub('\.fq\.gz',"_%s"%(trim_target_ext),fq_rev_trimmed)

#fq_fwd_se_sing = re.sub('\.fq\.gz',"_%s"%(trim_target_ext),fq_fwd_trimmed_sing)
#fq_rev_se_sing = re.sub('\.fq\.gz',"_%s"%(trim_target_ext),fq_rev_trimmed_sing)

fq_fwd_se = re.sub(sample_file_ext,"_%s"%(trim_target_ext),fq_fwd_trimmed)
fq_rev_se = re.sub(sample_file_ext,"_%s"%(trim_target_ext),fq_rev_trimmed)

fq_fwd_se_sing = re.sub(sample_file_ext,"_%s"%(trim_target_ext),fq_fwd_trimmed_sing)
fq_rev_se_sing = re.sub(sample_file_ext,"_%s"%(trim_target_ext),fq_rev_trimmed_sing)


quality_output = [fq_fwd_trimmed, fq_fwd_se,
                  fq_rev_trimmed, fq_rev_se]

quality_output_sing = [fq_fwd_trimmed_sing, fq_fwd_se_sing,
                  fq_rev_trimmed_sing, fq_rev_se_sing]
                 
adapter_file = join(data_dir, readfilt['adapter_file']['name'])
adapter_file_sing = join(sing_dir, readfilt['adapter_file']['name'])

quality_log = join(data_dir,'{sample}_trim{qual}_trimmomatic_pe.log')
quality_log_sing = join(sing_dir,'{sample}_trim{qual}_trimmomatic_pe.log')

trimmo_image = container_image_name(biocontainers,'trimmomatic')


def quality_trimming_quality_log_sing(wildcards):
    """Get the log file for this quality trimming param set"""
    return quality_log_sing.format(**wildcards)

def quality_trimming_quality_input(wildcards):
    """
    Wildcard substitution to get input files for trimming. 
    Absolute path for container: data/ becomes /data/
    """

    # input already includes data/ prefix
    quality_input_wc = [x.format(**wildcards) for x in quality_input_sing]

    # for containers, turn prefix data/ into /data/
    quality_input_wc = ["/%s"%(x) for x in quality_input_wc]
    quality_input_wcs = " ".join(quality_input_wc)
    return quality_input_wcs

def quality_trimming_quality_output(wildcards):
    """
    Wildcard substitution to get input files for trimming. 
    Absolute path for container: data/ becomes /data/
    """
    # input includes data/ prefix
    quality_output_wc = [x.format(**wildcards) for x in quality_output_sing]

    # for containers, turn prefix data/ into /data/
    quality_output_wc = ["/%s"%(x) for x in quality_output_wc]
    quality_output_wcs = " ".join(quality_output_wc)
    return quality_output_wcs


rule quality_trimming:
    """
    Trim reads from the sequencer by dropping low-quality reads.
    """
    input:
        quality_input, adapter_file
    output:
        quality_output
    message: 
        """--- Quality trimming read data."""
    singularity: 
        trimmo_image
    threads: 
        trimmmo_threads
    params:
        qual = quality_trimming_qual,
        quality_input_wc = quality_trimming_quality_input,
        quality_output_wc = quality_trimming_quality_output,
        quality_log_wc = quality_trimming_quality_log_sing
    log: 
        quality_log
    shell:
        'trimmomatic PE '
        '-trimlog {params.quality_log_wc} '
        '-threads {threads} '
        '{params.quality_input_wc} '
        '{params.quality_output_wc} '
        'ILLUMINACLIP:/{adapter_file_sing}:2:30:15:1:true '
        'LEADING:{params.qual} '
        'TRAILING:{params.qual} '
        'SLIDINGWINDOW:4:{params.qual} '
        'MINLEN:25 '



###################################
#read filtering: multiqc statistics

read_filtering_multiqc_output_file_sing = join(sing_dir, readfilt['multiqc']['multiqc_pattern_report_file'])
read_filtering_multiqc_output_dir = join(data_dir, readfilt['multiqc']['multiqc_pattern_report'])
read_filtering_multiqc_output_dir_sing = join(sing_dir, readfilt['multiqc']['multiqc_pattern_report'])
    
multiqc_image = container_image_name(biocontainers, 'multiqc')

def read_filtering_expand_multiqc_input(wildcards):
    """
    Return the expanded list from fastqc files
    """
    multiqc_input_pattern_fwd = join(data_dir, readfilt['multiqc']['multiqc_input_pattern'].format(
                                   direction=readfilt['direction_labels']['forward'], 
                                   sample='{sample}', qual='{qual}'))
    multiqc_input_pattern_rev = join(data_dir, readfilt['multiqc']['multiqc_input_pattern'].format(
                                   direction=readfilt['direction_labels']['reverse'], 
                                   sample='{sample}', qual='{qual}'))
    wildcards.qual = workflows['read_filtering_posttrim_workflow']['qual']
    read_filtering_multiqc_input_pattern_fwd = expand(multiqc_input_pattern_fwd, sample = wildcards.sample, qual=wildcards.qual)
    read_filtering_multiqc_input_pattern_rev = expand(multiqc_input_pattern_rev, sample = wildcards.sample, qual=wildcards.qual)
    original_reads = expand(pre_trimming_outputs, sample = wildcards.sample)
    read_filtering_inputs_sing = [read_filtering_multiqc_input_pattern_fwd, read_filtering_multiqc_input_pattern_rev]  
    multiqc_inputs = read_filtering_multiqc_input_pattern_fwd + read_filtering_multiqc_input_pattern_rev + original_reads
    return multiqc_inputs


def read_filtering_expand_multiqc_input_sing(wildcards):
    """
    Return the expanded list from fastqc files inside sing image
    """
   
    multiqc_input_pattern_fwd = join(sing_dir, readfilt['multiqc']['multiqc_input_pattern'].format(
                                   direction=readfilt['direction_labels']['forward'], 
                                   sample='{sample}', qual='{qual}'))
    multiqc_input_pattern_rev = join(sing_dir, readfilt['multiqc']['multiqc_input_pattern'].format(
                                   direction=readfilt['direction_labels']['reverse'], 
                                   sample='{sample}', qual='{qual}'))
    
    wildcards.qual = workflows['read_filtering_posttrim_workflow']['qual']
    read_filtering_multiqc_input_pattern_fwd = expand(multiqc_input_pattern_fwd, sample = wildcards.sample, qual=wildcards.qual)
    read_filtering_multiqc_input_pattern_rev = expand(multiqc_input_pattern_rev, sample = wildcards.sample, qual=wildcards.qual)
    read_filtering_inputs_sing = [read_filtering_multiqc_input_pattern_fwd, read_filtering_multiqc_input_pattern_rev]
    original_reads = pre_trimming_qa_outputs(wildcards)  #calling from read_filtering pretrim    
    original_reads_list = list(original_reads.split(" "))    
    multiqc_inputs = read_filtering_multiqc_input_pattern_fwd + read_filtering_multiqc_input_pattern_rev + original_reads_list
    return multiqc_inputs
    

rule read_filtering_statistics_multiqc:
    """
    Compute read filtering statistics with multiqc
    """
    input:
        read_filtering_expand_multiqc_input
    output:
        directory(read_filtering_multiqc_output_dir)
    params:
        input_files = read_filtering_expand_multiqc_input_sing,
        output_files = read_filtering_multiqc_output_file_sing,
        output_dir = read_filtering_multiqc_output_dir_sing
    message: 
        '--- Compiling read filtering statistics with multiqc'
    singularity:
        multiqc_image
    shell:
        'multiqc {params.input_files} -n {params.output_files} -o {params.output_dir}'
        
### ############################################
### # Read Filtering: bypass trimming, download trimmed data directly
### #
### # THIS RULE SHOULD PROBABLY BE COMMENTED OUT
### #
### # Note: either quality_trimming or download_trimmed_data
### # must be enabled, but not both. Otherwise you get conflicts
### # due to two rules producing the same output file.
### # 
### # download_trimmed_data is for testing, folks will not normally
### # have already-trimmed data to download.
### 
### post_trimmed_pattern = readfilt['read_patterns']['post_trimming_pattern']
### post_trimmed_relative_path = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'])
### 
### def download_reads_trimmed_data_url(wildcards):
###     """
###     Given a set of wildcards, return the URL where the 
###     post-trimmed reads can be downloaded (if available).
###     """
###     # Get the filename only from the relative path, and do wildcard substitution
###     post_trimmed_name = post_trimmed_pattern.format(**wildcards)
### 
###     # Get the URL where this file is available
###     read_data_url = config['files'][post_trimmed_name]
### 
###     return read_data_url
### 
### def download_reads_trimmed_data_file(wildcards):
###     """
###     Return the post-trimming file that matches the given wildcards
###     """
###     # Get the relative path and do wildcard substitution
###     post_trimming_file = join(data_dir, readfilt['read_patterns']['post_trimming_pattern'])
###     return post_trimming_file.format(**wildcards)
### 
### 
### rule download_trimmed_data:
###     """
###     Fetch user-requested files from OSF containing trimmed reads that will be
###     used in various workflows.
### 
###     Note that this defines wildcard-based download rules, rather than
###     downloading all files all at once, to keep things flexible and fast.
###     """
###     output:
###         post_trimmed_relative_path
###     message:
###         """--- Skipping read trimming step, downloading trimmed reads directly."""
###     params:
###         trimmed_data_url = download_reads_trimmed_data_url,
###         trimmed_data_file = download_reads_trimmed_data_file
###     shell:
###         'wget -O {params.read_data_file} {params.read_data_url}'



###################################
# Read Filtering: build rules


directions = [readfilt['direction_labels']['forward'],
              readfilt['direction_labels']['reverse']]



rule read_filtering_pretrim_workflow:
    """
    Build rule: trigger the read filtering workflow
    """
    input:
        expand( pre_trimming_outputs,
                sample    = sample_input_files,
                direction = directions,
        )

rule read_filtering_posttrim_workflow:
    """
    Build rule: trigger the read filtering workflow
    """
    input:
        expand( post_trimming_outputs,
                sample    = sample_input_files,
                qual      = workflows['read_filtering_posttrim_workflow']['qual'],
                direction = directions, 
        )

rule read_filtering_khmer_interleave_reads_workflow:
    '''
    Build rule: Take forward and reversed trimmed reads and run interleave-reads
    '''
    input:
        expand( interleave_output,
                sample    = sample_input_files,
                direction = directions,
                qual      = workflows['read_filtering_khmer_interleave_reads_workflow']['qual'],
        )

rule read_filtering_khmer_subsample_interleaved_reads_workflow:
    '''
    Build rule: take interleaved reads input and run sample-reads-randomly
    '''
    input:
        expand( subsample_interleave_output,
                sample    = sample_input_files,
                qual      = workflows['read_filtering_khmer_subsample_interleaved_reads_workflow']['qual'],
        )

rule read_filtering_khmer_split_interleaved_reads_workflow:
    '''
    Build rules: Take interleaved input and run split-paired-reads
    '''
    input:
        expand( split_interleave_output,
                sample    = sample_input_files,
                qual        = workflows['read_filtering_khmer_split_interleaved_reads_workflow']['qual'],
                direction = directions,
        )

rule read_filtering_khmer_count_unique_kmers_workflow:
    '''
    Build rule: run unique-kmers python script return txt file
    '''
    input:
        expand( count_unique_reads_output,
                sample    = sample_input_files,
                qual        = workflows['read_filtering_khmer_count_unique_kmers_workflow']['qual'],
                kmers       = workflows['read_filtering_khmer_count_unique_kmers_workflow']['kmers'],
        )

rule read_filtering_multiqc_workflow:
    """
    Build rule: run multiqc stats on trimmed reads
    """
    input:
        expand( read_filtering_multiqc_output_dir,
                sample    = sample_input_files,
        )


rule read_filtering_fastq_to_fasta_workflow:
    '''
    build rule: convert fq.gz files to .fa files
    '''
    input:  
        expand("data/{file}.fa", file=files)


rule read_filtering_convert_PE_to_SE_workflow:
    input:
        expand( combine_read_trim_output,
            sample    = sample_input_files,
            qual      = workflows['read_filtering_convert_PE_to_SE_workflow']['qual']
        )
###################################
# Read Filtering: Low complexity filtering
# same as post trimmed inputs
# only used for seqscreen, so outputs not used elsewhere, yet

#inputs
lc_filter_reads_input = join(data_dir, readfilt['low_complexity_pattern_PE']['input_pattern'])
lc_filter_reads_input_sing = join(sing_dir, readfilt['low_complexity_pattern_PE']['input_pattern'])

#outputs
lc_filter_reads_output= join(data_dir, readfilt['low_complexity_pattern_PE']['output_pattern'])
lc_filter_reads_output_sing = join(sing_dir, readfilt['low_complexity_pattern_PE']['output_pattern'])

#html and json outs
lc_filter_html_sing = join(sing_dir, readfilt['low_complexity_pattern_PE']['html'])
lc_filter_html = join(data_dir, readfilt['low_complexity_pattern_PE']['html'])

lc_filter_json_sing = join(sing_dir, readfilt['low_complexity_pattern_PE']['json'])
lc_filter_json = join(data_dir, readfilt['low_complexity_pattern_PE']['json'])



def convert_lc_filter_input_sing(wildcards):
    return lc_filter_reads_input_sing.format(**wildcards)

def convert_lc_filter_input(wildcards):
    return lc_filter_reads_input_sing.format(**wildcards)

def convert_lc_filter_output_sing(wildcards):
    return lc_filter_reads_output_sing.format(**wildcards)

def convert_lc_filter_output(wildcards):
    return lc_filter_reads_output_sing.format(**wildcards)

#html convert
def convert_lc_filter_html_sing(wildcards):
    return lc_filter_html_sing.format(**wildcards)

def convert_lc_filter_html(wildcards):
    return lc_filter_html.format(**wildcards)

#json convert
def convert_lc_filter_json_sing(wildcards):
    return lc_filter_json_sing.format(**wildcards)

def convert_lc_filter_json(wildcards):
    return lc_filter_reads_json.format(**wildcards)

fastp_image = container_image_name(biocontainers,'fastp')

rule read_filtering_low_complexity_workflow:
    input:
        expand( lc_filter_reads_output,
            sample    = sample_input_files,
            qual      = workflows['read_filtering_low_complexity_workflow']['qual'],
        )


rule lc_filter_reads:
    input:
        lc_filter_reads_input
    output:
        lc_filter_reads_output
    params:
        filter_input = convert_lc_filter_input_sing,
        filter_output = convert_lc_filter_output_sing,
        filter_html = convert_lc_filter_html_sing,
        filter_json = convert_lc_filter_json_sing
    singularity: 
        fastp_image
    shell:
        "fastp -i {params.filter_input} -o {params.filter_output} --low_complexity_filter --complexity_threshold 20 --disable_quality_filtering --disable_adapter_trimming --thread {threads} --html {params.filter_html} --json {params.filter_json}"

